


#### Got First pass on the feed forward today 

```python 
from sklearn.metrics import log_loss

import network as n

X, Y = n.build_dataset_inside_outside_circle()


In [22]: x = X[0]
    ...: n.feed_forward(x, n.layers)
0 Layer(weights=array([[0.79468653, 0.52103056, 0.34222824],
       [0.11356033, 0.98920516, 0.10486598]]), bias=array([1]))
sizes: (3,) (3, 3)
output of relu [0 0 0]
1 Layer(weights=array([[0.44784533, 0.15159785],
       [0.19089017, 0.64243756],
       [0.95099016, 0.52408949]]), bias=array([1]))
sizes: (4,) (4, 2)
output of relu [1. 1.]
2 Layer(weights=array([[1],
       [1]]), bias=array([0]))
sizes: (3,) (3, 1)
output of relu [2.]
Out[22]: 2.0


Y_hat = np.array(list(map(lambda x: n.feed_forward(x, n.layers), X)))
```

#### And the log loss, 
```python 
log_loss(Y, Y_hat, labels=[0, 1])
# Out[54]: 32.86786051431157
```
Ok cool going to consider this like the base case log loss, that we get, without any training, with randomized weights. 
