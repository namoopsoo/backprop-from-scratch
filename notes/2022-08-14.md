

### 2022-08-14
#### Got First pass on the feed forward today 

```python 
from sklearn.metrics import log_loss

import network as n

X, Y = n.build_dataset_inside_outside_circle()


In [22]: x = X[0]
    ...: n.feed_forward(x, n.layers)
0 Layer(weights=array([[0.79468653, 0.52103056, 0.34222824],
       [0.11356033, 0.98920516, 0.10486598]]), bias=array([1]))
sizes: (3,) (3, 3)
output of relu [0 0 0]
1 Layer(weights=array([[0.44784533, 0.15159785],
       [0.19089017, 0.64243756],
       [0.95099016, 0.52408949]]), bias=array([1]))
sizes: (4,) (4, 2)
output of relu [1. 1.]
2 Layer(weights=array([[1],
       [1]]), bias=array([0]))
sizes: (3,) (3, 1)
output of relu [2.]
Out[22]: 2.0


Y_hat = np.array(list(map(lambda x: n.feed_forward(x, n.layers), X)))
```

#### And the log loss, 
```python 
log_loss(Y, Y_hat, labels=[0, 1])
# Out[54]: 32.86786051431157
```
Ok cool going to consider this like the base case log loss, that we get, without any training, with randomized weights. 

### 2022-08-15

#### Implemented first stab at storage of values
Have some super rough player in each layer for storing the net and activation values at each layer.

And manually wrote out the partial derivatives for two weights as well as wrote a super rough train loop, 
for updating those two weights, w13 and w14, when randomly sampling a single-example mini-batch.

#### Stopped today , 
with at one point getting a `nan` output of the `feed_forward` so will next have to see why that is.



### 2022-08-16

#### why that y_prob nan? 

hmm dumped a debug from last night, 

```python 

ipdb> p y, y_prob
(0, nan)
ipdb> p layers
[Layer(weights=array([[0.41567757, 0.11380143, 0.75724346],
       [0.86557157, 0.05242256, 0.56130094]]), bias=array([1]), nodes={'net_h1': -10.483523248075883, 'h1': 0, 'net_h2': -0.1541754467969254, 'h2': 0, 'net_h3': -8.970799858978893, 'h3': 0}), Layer(weights=array([[0.4220101 , 0.05824487],
       [0.79738675, 0.96748173],
       [0.39207329, 0.61591024]]), bias=array([1]), nodes={'net_h4': 1.0, 'h4': 1.0, 'net_h5': 1.0, 'h5': 1.0}), Layer(weights=array([[inf],
       [inf]]), bias=array([0]), nodes={'net_y_logit': inf, 'y_logit': inf, 'y_prob': nan})]
ipdb> pp layers
[Layer(weights=array([[0.41567757, 0.11380143, 0.75724346],
       [0.86557157, 0.05242256, 0.56130094]]), bias=array([1]), nodes={'net_h1': -10.483523248075883, 'h1': 0, 'net_h2': -0.1541754467969254, 'h2': 0, 'net_h3': -8.970799858978893, 'h3': 0}),
 Layer(weights=array([[0.4220101 , 0.05824487],
       [0.79738675, 0.96748173],
       [0.39207329, 0.61591024]]), bias=array([1]), nodes={'net_h4': 1.0, 'h4': 1.0, 'net_h5': 1.0, 'h5': 1.0}),
 Layer(weights=array([[inf],
       [inf]]), bias=array([0]), nodes={'net_y_logit': inf, 'y_logit': inf, 'y_prob': nan})]
ipdb> 
```
Looks like `y_prob` is `nan` because `net_y_logit` is `inf` hah. And that's because I see the weights on last layer are `inf` . 


#### derivative of log loss undefined for y_prob 0?
This set trace I left here got tripped then hmm
```python 
def derivative_of_log_loss(y, y_prob):
    # Note, using y_hat and y_prob interchangeably
    result = -y / y_prob - (1 - y) / (1 - y_prob)

    if y_prob == 1:
        ipdb.set_trace()

    return result

```
```python 
ipdb> l
     91     result = -y / y_prob - (1 - y) / (1 - y_prob)
     92 
     93     if y_prob == 1:
     94         ipdb.set_trace()
     95 
---> 96     return result
     97 
     98 
     99 
    100 def concat_bias_weights(x):
    101     num_cols = x.shape[1]

ipdb> p result
-inf
ipdb> 
```

Ok so next thing is to look at that then. 
