

### 2022-08-14
#### Got First pass on the feed forward today 

```python 
from sklearn.metrics import log_loss

import network as n

X, Y = n.build_dataset_inside_outside_circle()


In [22]: x = X[0]
    ...: n.feed_forward(x, n.layers)
0 Layer(weights=array([[0.79468653, 0.52103056, 0.34222824],
       [0.11356033, 0.98920516, 0.10486598]]), bias=array([1]))
sizes: (3,) (3, 3)
output of relu [0 0 0]
1 Layer(weights=array([[0.44784533, 0.15159785],
       [0.19089017, 0.64243756],
       [0.95099016, 0.52408949]]), bias=array([1]))
sizes: (4,) (4, 2)
output of relu [1. 1.]
2 Layer(weights=array([[1],
       [1]]), bias=array([0]))
sizes: (3,) (3, 1)
output of relu [2.]
Out[22]: 2.0


Y_hat = np.array(list(map(lambda x: n.feed_forward(x, n.layers), X)))
```

#### And the log loss, 
```python 
log_loss(Y, Y_hat, labels=[0, 1])
# Out[54]: 32.86786051431157
```
Ok cool going to consider this like the base case log loss, that we get, without any training, with randomized weights. 

### 2022-08-15

#### Implemented first stab at storage of values
Have some super rough player in each layer for storing the net and activation values at each layer.

And manually wrote out the partial derivatives for two weights as well as wrote a super rough train loop, 
for updating those two weights, w13 and w14, when randomly sampling a single-example mini-batch.

#### Stopped today , 
with at one point getting a `nan` output of the `feed_forward` so will next have to see why that is.
